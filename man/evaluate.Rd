% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluate.R
\name{evaluate}
\alias{evaluate}
\title{Evaluate Model Performance}
\usage{
evaluate(y, y_pred)
}
\arguments{
\item{y}{True labels (ground truth) of the test data.}

\item{y_pred}{Predicted labels generated by the classification model.}
}
\value{
A list containing the accuracy and confusion matrix.
}
\description{
This function evaluates the performance of a classification model by calculating accuracy and generating a confusion matrix.
}
\examples{
data <- read.csv("test_data.csv")
y_true <- data$true_labels
y_pred <- predict_perceptron(model, data$x)
evaluation <- evaluate(y_true, y_pred)
print(evaluation$accuracy)
print(evaluation$conf_matrix)
}
